{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Variants Evaluation - Visualization and Analysis\n",
    "\n",
    "This notebook visualizes and analyzes the results from evaluating coherence-based hallucination detection variants on the wiki_bio_gpt3_hallucination dataset.\n",
    "\n",
    "## Features\n",
    "- Load evaluation results from JSON files\n",
    "- Plot ROC curves for all three variants\n",
    "- Plot Precision-Recall curves\n",
    "- Analyze score distributions by ground truth labels\n",
    "- Compare against SelfCheckAPIPrompt baseline\n",
    "- Per-sentence analysis for interesting cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Results\n",
    "\n",
    "Load the most recent evaluation results JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from: ../results/coherence_evaluation_20251103_180722.json\n",
      "\n",
      "Evaluation Metadata:\n",
      "  model: gpt-4o-mini\n",
      "  num_samples: 2\n",
      "  max_passages: 10\n",
      "  variants_evaluated: ['shogenji']\n",
      "  total_time_seconds: 269.4456639289856\n",
      "  baseline_auc_pr: 93.42\n",
      "\n",
      "Metrics Summary:\n",
      "  shogenji:\n",
      "    AUC-PR: 83.19%\n",
      "    PCC: -3.56%\n",
      "    AUC-ROC: 39.68%\n"
     ]
    }
   ],
   "source": [
    "# Find most recent results file\n",
    "results_dir = \"../results\"\n",
    "result_files = glob.glob(os.path.join(results_dir, \"coherence_evaluation_*.json\"))\n",
    "\n",
    "if not result_files:\n",
    "    print(\"No evaluation results found!\")\n",
    "    print(f\"Please run: python scripts/evaluate_coherence.py --variant all\")\n",
    "else:\n",
    "    # Load most recent file\n",
    "    latest_file = max(result_files, key=os.path.getctime)\n",
    "    print(f\"Loading results from: {latest_file}\")\n",
    "    \n",
    "    with open(latest_file, 'r') as f:\n",
    "        eval_results = json.load(f)\n",
    "    \n",
    "    print(\"\\nEvaluation Metadata:\")\n",
    "    for key, value in eval_results['metadata'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    for variant, metrics in eval_results['results'].items():\n",
    "        print(f\"  {variant}:\")\n",
    "        print(f\"    AUC-PR: {metrics['auc_pr']*100:.2f}%\")\n",
    "        print(f\"    PCC: {metrics['pcc']*100:.2f}%\")\n",
    "        print(f\"    AUC-ROC: {metrics['auc_roc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run Evaluation to Get Detailed Scores\n",
    "\n",
    "To create visualizations, we need the actual score arrays. Let's re-run a small subset evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "Configuration:\n",
      "  Model: gpt-4o-mini\n",
      "  Num samples: 3\n",
      "  Max passages: 50\n",
      "  Total passages in dataset: 238\n"
     ]
    }
   ],
   "source": [
    "from selfcheckgpt.modeling_coherence import SelfCheckShogenji, SelfCheckFitelson, SelfCheckOlsson\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"potsawee/wiki_bio_gpt3_hallucination\")[\"evaluation\"]\n",
    "\n",
    "# Configuration\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "NUM_SAMPLES = 3\n",
    "MAX_PASSAGES = 50  # Evaluate subset for faster visualization\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Num samples: {NUM_SAMPLES}\")\n",
    "print(f\"  Max passages: {MAX_PASSAGES}\")\n",
    "print(f\"  Total passages in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing coherence variants...\n",
      "Initiate OpenAI client for coherence detection... model = gpt-4o-mini\n",
      "Initiate OpenAI client for coherence detection... model = gpt-4o-mini\n",
      "Initiate OpenAI client for coherence detection... model = gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Initialize variants\n",
    "print(\"Initializing coherence variants...\")\n",
    "selfcheck_shogenji = SelfCheckShogenji(model=MODEL)\n",
    "selfcheck_fitelson = SelfCheckFitelson(model=MODEL)\n",
    "selfcheck_olsson = SelfCheckOlsson(model=MODEL)\n",
    "\n",
    "variants = {\n",
    "    'Shogenji': selfcheck_shogenji,\n",
    "    'Fitelson': selfcheck_fitelson,\n",
    "    'Olsson': selfcheck_olsson\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7871280478d4f18b2ff33f309f229ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating passages:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathanlubchenco/workspace/selfcheckgpt/selfcheckgpt/utils_coherence.py:81: RuntimeWarning: Found 1 cases where P(A∧B) > P(A) or P(A∧B) > P(B). This violates probability axioms. Clamping P(A∧B) to min(P(A), P(B)).\n",
      "  warnings.warn(\n",
      "/Users/nathanlubchenco/workspace/selfcheckgpt/selfcheckgpt/utils_coherence.py:150: RuntimeWarning: Found 1 cases where P(A∧B) > P(A) or P(A∧B) > P(B). This violates probability axioms. Clamping P(A∧B) to min(P(A), P(B)).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete!\n",
      "Total sentences: 423\n",
      "Accurate: 0, Inaccurate: 0\n"
     ]
    }
   ],
   "source": [
    "# Collect scores for all variants\n",
    "variant_scores = {name: [] for name in variants.keys()}\n",
    "all_labels = []\n",
    "\n",
    "num_eval_passages = min(MAX_PASSAGES, len(dataset))\n",
    "\n",
    "for passage_idx in tqdm(range(num_eval_passages), desc=\"Evaluating passages\"):\n",
    "    passage_data = dataset[passage_idx]\n",
    "    \n",
    "    sentences = passage_data['gpt3_sentences']\n",
    "    annotations = passage_data['annotation']\n",
    "    gpt3_text = passage_data['gpt3_text']\n",
    "    \n",
    "    # Create sampled passages\n",
    "    sampled_passages = [gpt3_text] * NUM_SAMPLES\n",
    "    \n",
    "    # Evaluate with each variant\n",
    "    for variant_name, variant in variants.items():\n",
    "        try:\n",
    "            scores = variant.predict(\n",
    "                sentences=sentences,\n",
    "                sampled_passages=sampled_passages,\n",
    "                verbose=False\n",
    "            )\n",
    "            variant_scores[variant_name].extend(scores.tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {variant_name} on passage {passage_idx}: {e}\")\n",
    "            # Pad with zeros\n",
    "            variant_scores[variant_name].extend([0.0] * len(sentences))\n",
    "    \n",
    "    # Collect labels once\n",
    "    if passage_idx == 0 or len(all_labels) < sum(len(variant_scores[name]) for name in variants.keys()) / len(variants):\n",
    "        all_labels.extend(annotations)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "for variant_name in variant_scores:\n",
    "    variant_scores[variant_name] = np.array(variant_scores[variant_name])\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(f\"\\nEvaluation complete!\")\n",
    "print(f\"Total sentences: {len(all_labels)}\")\n",
    "print(f\"Accurate: {np.sum(all_labels == 0)}, Inaccurate: {np.sum(all_labels == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "Plot Receiver Operating Characteristic curves for all three variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m colors = {\u001b[33m'\u001b[39m\u001b[33mShogenji\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFitelson\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mgreen\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mOlsson\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mred\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variant_name, scores \u001b[38;5;129;01min\u001b[39;00m variant_scores.items():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     fpr, tpr, _ = \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     auc_roc = roc_auc_score(all_labels, scores)\n\u001b[32m      9\u001b[39m     plt.plot(\n\u001b[32m     10\u001b[39m         fpr, tpr,\n\u001b[32m     11\u001b[39m         label=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (AUC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc_roc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m         color=colors[variant_name],\n\u001b[32m     13\u001b[39m         linewidth=\u001b[32m2\u001b[39m\n\u001b[32m     14\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/selfcheckgpt/venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/selfcheckgpt/venv/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:1163\u001b[39m, in \u001b[36mroc_curve\u001b[39m\u001b[34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[39m\n\u001b[32m   1059\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   1060\u001b[39m     {\n\u001b[32m   1061\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1070\u001b[39m     y_true, y_score, *, pos_label=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1071\u001b[39m ):\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[32m   1073\u001b[39m \n\u001b[32m   1074\u001b[39m \u001b[33;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1161\u001b[39m \u001b[33;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[32m   1162\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1163\u001b[39m     fps, tps, thresholds = \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[32m   1169\u001b[39m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1174\u001b[39m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[32m   1175\u001b[39m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[32m   1176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) > \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/selfcheckgpt/venv/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:863\u001b[39m, in \u001b[36m_binary_clf_curve\u001b[39m\u001b[34m(y_true, y_score, pos_label, sample_weight)\u001b[39m\n\u001b[32m    861\u001b[39m y_type = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type == \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m format is not supported\u001b[39m\u001b[33m\"\u001b[39m.format(y_type))\n\u001b[32m    865\u001b[39m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[32m    866\u001b[39m y_true = column_or_1d(y_true)\n",
      "\u001b[31mValueError\u001b[39m: multiclass format is not supported"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = {'Shogenji': 'blue', 'Fitelson': 'green', 'Olsson': 'red'}\n",
    "\n",
    "for variant_name, scores in variant_scores.items():\n",
    "    fpr, tpr, _ = roc_curve(all_labels, scores)\n",
    "    auc_roc = roc_auc_score(all_labels, scores)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr, tpr,\n",
    "        label=f'{variant_name} (AUC={auc_roc:.3f})',\n",
    "        color=colors[variant_name],\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "# Diagonal reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Baseline')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves: Coherence-Based Hallucination Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curves\n",
    "\n",
    "Plot Precision-Recall curves showing performance at different operating points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for variant_name, scores in variant_scores.items():\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, scores)\n",
    "    auc_pr = average_precision_score(all_labels, scores)\n",
    "    \n",
    "    plt.plot(\n",
    "        recall, precision,\n",
    "        label=f'{variant_name} (AP={auc_pr:.3f})',\n",
    "        color=colors[variant_name],\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "# Baseline\n",
    "baseline = np.sum(all_labels == 1) / len(all_labels)\n",
    "plt.axhline(y=baseline, color='k', linestyle='--', linewidth=1, label=f'Random Baseline ({baseline:.3f})')\n",
    "\n",
    "# Add reference line for SelfCheckAPIPrompt baseline (93.42 AP)\n",
    "plt.axhline(y=0.9342, color='purple', linestyle=':', linewidth=2, label='SelfCheckAPIPrompt Baseline (0.934)')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves: Coherence-Based Hallucination Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower left', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distributions by Ground Truth Label\n",
    "\n",
    "Visualize how well each variant separates accurate from inaccurate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (variant_name, scores) in enumerate(variant_scores.items()):\n",
    "    # Separate scores by label\n",
    "    accurate_scores = scores[all_labels == 0]\n",
    "    inaccurate_scores = scores[all_labels == 1]\n",
    "    \n",
    "    # Plot histograms\n",
    "    axes[idx].hist(\n",
    "        accurate_scores,\n",
    "        bins=30,\n",
    "        alpha=0.6,\n",
    "        color='green',\n",
    "        label=f'Accurate (n={len(accurate_scores)})',\n",
    "        density=True\n",
    "    )\n",
    "    axes[idx].hist(\n",
    "        inaccurate_scores,\n",
    "        bins=30,\n",
    "        alpha=0.6,\n",
    "        color='red',\n",
    "        label=f'Inaccurate (n={len(inaccurate_scores)})',\n",
    "        density=True\n",
    "    )\n",
    "    \n",
    "    axes[idx].set_xlabel('Hallucination Score', fontsize=11)\n",
    "    axes[idx].set_ylabel('Density', fontsize=11)\n",
    "    axes[idx].set_title(f'{variant_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(fontsize=10)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Add mean lines\n",
    "    axes[idx].axvline(\n",
    "        np.mean(accurate_scores),\n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        label=f'Mean Accurate: {np.mean(accurate_scores):.3f}'\n",
    "    )\n",
    "    axes[idx].axvline(\n",
    "        np.mean(inaccurate_scores),\n",
    "        color='red',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        label=f'Mean Inaccurate: {np.mean(inaccurate_scores):.3f}'\n",
    "    )\n",
    "\n",
    "plt.suptitle('Hallucination Score Distributions by Ground Truth', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print separation statistics\n",
    "print(\"\\nSeparation Statistics (Mean Inaccurate - Mean Accurate):\")\n",
    "for variant_name, scores in variant_scores.items():\n",
    "    accurate_mean = np.mean(scores[all_labels == 0])\n",
    "    inaccurate_mean = np.mean(scores[all_labels == 1])\n",
    "    separation = inaccurate_mean - accurate_mean\n",
    "    print(f\"  {variant_name}: {separation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Sentence Analysis: Interesting Cases\n",
    "\n",
    "Examine cases where variants disagree or show unusual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cases where variants disagree significantly\n",
    "shogenji_scores = variant_scores['Shogenji']\n",
    "fitelson_scores = variant_scores['Fitelson']\n",
    "olsson_scores = variant_scores['Olsson']\n",
    "\n",
    "# Calculate score differences\n",
    "shog_fitel_diff = np.abs(shogenji_scores - fitelson_scores)\n",
    "shog_olsson_diff = np.abs(shogenji_scores - olsson_scores)\n",
    "fitel_olsson_diff = np.abs(fitelson_scores - olsson_scores)\n",
    "\n",
    "max_diff = np.maximum(np.maximum(shog_fitel_diff, shog_olsson_diff), fitel_olsson_diff)\n",
    "\n",
    "# Find top 10 disagreement cases\n",
    "disagreement_indices = np.argsort(max_diff)[-10:][::-1]\n",
    "\n",
    "print(\"Top 10 Cases with Highest Variant Disagreement:\\n\")\n",
    "print(f\"{'Index':<8} {'Label':<10} {'Shogenji':<12} {'Fitelson':<12} {'Olsson':<12} {'Max Diff':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx in disagreement_indices:\n",
    "    label_str = \"Accurate\" if all_labels[idx] == 0 else \"Inaccurate\"\n",
    "    print(\n",
    "        f\"{idx:<8} {label_str:<10} \"\n",
    "        f\"{shogenji_scores[idx]:<12.4f} \"\n",
    "        f\"{fitelson_scores[idx]:<12.4f} \"\n",
    "        f\"{olsson_scores[idx]:<12.4f} \"\n",
    "        f\"{max_diff[idx]:<12.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find false negatives (inaccurate sentences with low scores)\n",
    "inaccurate_indices = np.where(all_labels == 1)[0]\n",
    "avg_scores = (shogenji_scores + fitelson_scores + olsson_scores) / 3\n",
    "\n",
    "false_negative_candidates = [(idx, avg_scores[idx]) for idx in inaccurate_indices]\n",
    "false_negative_candidates.sort(key=lambda x: x[1])  # Sort by score (ascending)\n",
    "\n",
    "print(\"\\nTop 5 Potential False Negatives (Inaccurate with Low Scores):\\n\")\n",
    "print(f\"{'Index':<8} {'Avg Score':<12} {'Shogenji':<12} {'Fitelson':<12} {'Olsson':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, avg_score in false_negative_candidates[:5]:\n",
    "    print(\n",
    "        f\"{idx:<8} {avg_score:<12.4f} \"\n",
    "        f\"{shogenji_scores[idx]:<12.4f} \"\n",
    "        f\"{fitelson_scores[idx]:<12.4f} \"\n",
    "        f\"{olsson_scores[idx]:<12.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find false positives (accurate sentences with high scores)\n",
    "accurate_indices = np.where(all_labels == 0)[0]\n",
    "\n",
    "false_positive_candidates = [(idx, avg_scores[idx]) for idx in accurate_indices]\n",
    "false_positive_candidates.sort(key=lambda x: x[1], reverse=True)  # Sort by score (descending)\n",
    "\n",
    "print(\"\\nTop 5 Potential False Positives (Accurate with High Scores):\\n\")\n",
    "print(f\"{'Index':<8} {'Avg Score':<12} {'Shogenji':<12} {'Fitelson':<12} {'Olsson':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, avg_score in false_positive_candidates[:5]:\n",
    "    print(\n",
    "        f\"{idx:<8} {avg_score:<12.4f} \"\n",
    "        f\"{shogenji_scores[idx]:<12.4f} \"\n",
    "        f\"{fitelson_scores[idx]:<12.4f} \"\n",
    "        f\"{olsson_scores[idx]:<12.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Baseline\n",
    "\n",
    "Compare coherence variants to the SelfCheckAPIPrompt (GPT-3.5) baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline metrics from paper\n",
    "baseline_auc_pr = 0.9342\n",
    "baseline_pcc = 0.7832\n",
    "\n",
    "# Calculate metrics for each variant\n",
    "variant_metrics = {}\n",
    "for variant_name, scores in variant_scores.items():\n",
    "    from scipy.stats import pearsonr\n",
    "    \n",
    "    auc_pr = average_precision_score(all_labels, scores)\n",
    "    pcc, _ = pearsonr(scores, all_labels)\n",
    "    auc_roc = roc_auc_score(all_labels, scores)\n",
    "    \n",
    "    variant_metrics[variant_name] = {\n",
    "        'auc_pr': auc_pr,\n",
    "        'pcc': pcc,\n",
    "        'auc_roc': auc_roc\n",
    "    }\n",
    "\n",
    "# Create comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "variants_list = list(variant_metrics.keys()) + ['Baseline (GPT-3.5)']\n",
    "x = np.arange(len(variants_list))\n",
    "\n",
    "# AUC-PR comparison\n",
    "auc_pr_values = [variant_metrics[v]['auc_pr'] * 100 for v in variant_metrics.keys()] + [baseline_auc_pr * 100]\n",
    "bars1 = axes[0].bar(x, auc_pr_values, color=['blue', 'green', 'red', 'purple'], alpha=0.7)\n",
    "axes[0].set_ylabel('AUC-PR (%)', fontsize=12)\n",
    "axes[0].set_title('AUC-PR Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(variants_list, rotation=15, ha='right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].axhline(y=baseline_auc_pr * 100, color='purple', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# PCC comparison\n",
    "pcc_values = [variant_metrics[v]['pcc'] * 100 for v in variant_metrics.keys()] + [baseline_pcc * 100]\n",
    "bars2 = axes[1].bar(x, pcc_values, color=['blue', 'green', 'red', 'purple'], alpha=0.7)\n",
    "axes[1].set_ylabel('PCC (%)', fontsize=12)\n",
    "axes[1].set_title('Pearson Correlation Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(variants_list, rotation=15, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].axhline(y=baseline_pcc * 100, color='purple', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.suptitle('Coherence Variants vs SelfCheckAPIPrompt Baseline', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\nDetailed Metrics Comparison:\\n\")\n",
    "print(f\"{'Variant':<20} {'AUC-PR':<12} {'vs Baseline':<15} {'PCC':<12} {'AUC-ROC':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for variant_name, metrics in variant_metrics.items():\n",
    "    diff_pr = (metrics['auc_pr'] - baseline_auc_pr) * 100\n",
    "    print(\n",
    "        f\"{variant_name:<20} \"\n",
    "        f\"{metrics['auc_pr']*100:<12.2f} \"\n",
    "        f\"{diff_pr:+.2f}%{' ':<10} \"\n",
    "        f\"{metrics['pcc']*100:<12.2f} \"\n",
    "        f\"{metrics['auc_roc']*100:<12.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    f\"{'Baseline (GPT-3.5)':<20} \"\n",
    "    f\"{baseline_auc_pr*100:<12.2f} \"\n",
    "    f\"{'---':<15} \"\n",
    "    f\"{baseline_pcc*100:<12.2f} \"\n",
    "    f\"{'---':<12}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "Key observations from the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Best performing variant\n",
    "best_variant = max(variant_metrics.items(), key=lambda x: x[1]['auc_pr'])\n",
    "print(f\"\\nBest Performing Variant (by AUC-PR): {best_variant[0]}\")\n",
    "print(f\"  AUC-PR: {best_variant[1]['auc_pr']*100:.2f}%\")\n",
    "print(f\"  Improvement over baseline: {(best_variant[1]['auc_pr'] - baseline_auc_pr)*100:+.2f}%\")\n",
    "\n",
    "# Consistency across variants\n",
    "auc_pr_std = np.std([m['auc_pr'] for m in variant_metrics.values()])\n",
    "print(f\"\\nVariant Consistency (AUC-PR std): {auc_pr_std*100:.2f}%\")\n",
    "\n",
    "# Cache efficiency\n",
    "if 'cache_stats' in eval_results:\n",
    "    print(\"\\nCache Efficiency:\")\n",
    "    for variant_name, stats in eval_results['cache_stats'].items():\n",
    "        print(f\"  {variant_name}: {stats['hit_rate']*100:.2f}% hit rate\")\n",
    "\n",
    "# Cost estimates\n",
    "if 'cost_estimates' in eval_results:\n",
    "    print(\"\\nCost Estimates:\")\n",
    "    total_cost = 0\n",
    "    for variant_name, costs in eval_results['cost_estimates'].items():\n",
    "        cost = costs['estimated_cost_usd']\n",
    "        total_cost += cost\n",
    "        print(f\"  {variant_name}: ${cost:.4f} USD\")\n",
    "    print(f\"  Total: ${total_cost:.4f} USD\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided comprehensive visualization and analysis of the coherence-based hallucination detection variants. The results show:\n",
    "\n",
    "1. **Performance**: How each variant compares to the SelfCheckAPIPrompt baseline\n",
    "2. **Discrimination**: How well variants separate accurate from inaccurate sentences\n",
    "3. **Consistency**: Where variants agree or disagree in their assessments\n",
    "4. **Efficiency**: Cache hit rates and API cost management\n",
    "\n",
    "For production deployment, consider:\n",
    "- The best-performing variant based on your metric priorities (AUC-PR vs PCC)\n",
    "- API cost constraints and caching strategies\n",
    "- Ensemble approaches combining multiple variants for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
