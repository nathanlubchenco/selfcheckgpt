{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Variants Evaluation - Visualization and Analysis\n",
    "\n",
    "This notebook visualizes and analyzes the results from evaluating coherence-based hallucination detection variants on the wiki_bio_gpt3_hallucination dataset.\n",
    "\n",
    "## Features\n",
    "- Load evaluation results from JSON files\n",
    "- Plot ROC curves for all three variants\n",
    "- Plot Precision-Recall curves\n",
    "- Analyze score distributions by ground truth labels\n",
    "- Compare against SelfCheckAPIPrompt baseline\n",
    "- Per-sentence analysis for interesting cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Results\n",
    "\n",
    "Load the most recent evaluation results JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most recent results file\n",
    "results_dir = \"../results\"\n",
    "result_files = glob.glob(os.path.join(results_dir, \"coherence_evaluation_*.json\"))\n",
    "\n",
    "if not result_files:\n",
    "    print(\"No evaluation results found!\")\n",
    "    print(f\"Please run: python scripts/evaluate_coherence.py --variant all\")\n",
    "else:\n",
    "    # Load most recent file\n",
    "    latest_file = max(result_files, key=os.path.getctime)\n",
    "    print(f\"Loading results from: {latest_file}\")\n",
    "    \n",
    "    with open(latest_file, 'r') as f:\n",
    "        eval_results = json.load(f)\n",
    "    \n",
    "    print(\"\\nEvaluation Metadata:\")\n",
    "    for key, value in eval_results['metadata'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    for variant, metrics in eval_results['results'].items():\n",
    "        print(f\"  {variant}:\")\n",
    "        print(f\"    AUC-PR: {metrics['auc_pr']*100:.2f}%\")\n",
    "        print(f\"    PCC: {metrics['pcc']*100:.2f}%\")\n",
    "        print(f\"    AUC-ROC: {metrics['auc_roc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run Evaluation to Get Detailed Scores\n",
    "\n",
    "To create visualizations, we need the actual score arrays. Let's re-run a small subset evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selfcheckgpt.modeling_coherence import SelfCheckShogenji, SelfCheckFitelson, SelfCheckOlsson\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"potsawee/wiki_bio_gpt3_hallucination\")[\"evaluation\"]\n",
    "\n",
    "# Configuration\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "NUM_SAMPLES = 3\n",
    "MAX_PASSAGES = 50  # Evaluate subset for faster visualization\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Num samples: {NUM_SAMPLES}\")\n",
    "print(f\"  Max passages: {MAX_PASSAGES}\")\n",
    "print(f\"  Total passages in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variants\n",
    "print(\"Initializing coherence variants...\")\n",
    "selfcheck_shogenji = SelfCheckShogenji(model=MODEL)\n",
    "selfcheck_fitelson = SelfCheckFitelson(model=MODEL)\n",
    "selfcheck_olsson = SelfCheckOlsson(model=MODEL)\n",
    "\n",
    "variants = {\n",
    "    'Shogenji': selfcheck_shogenji,\n",
    "    'Fitelson': selfcheck_fitelson,\n",
    "    'Olsson': selfcheck_olsson\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect scores for all variants\n",
    "variant_scores = {name: [] for name in variants.keys()}\n",
    "all_labels = []\n",
    "\n",
    "num_eval_passages = min(MAX_PASSAGES, len(dataset))\n",
    "\n",
    "for passage_idx in tqdm(range(num_eval_passages), desc=\"Evaluating passages\"):\n",
    "    passage_data = dataset[passage_idx]\n",
    "    \n",
    "    sentences = passage_data['gpt3_sentences']\n",
    "    annotations = passage_data['annotation']\n",
    "    gpt3_text = passage_data['gpt3_text']\n",
    "    \n",
    "    # Create sampled passages\n",
    "    sampled_passages = [gpt3_text] * NUM_SAMPLES\n",
    "    \n",
    "    # Evaluate with each variant\n",
    "    for variant_name, variant in variants.items():\n",
    "        try:\n",
    "            scores = variant.predict(\n",
    "                sentences=sentences,\n",
    "                sampled_passages=sampled_passages,\n",
    "                verbose=False\n",
    "            )\n",
    "            variant_scores[variant_name].extend(scores.tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {variant_name} on passage {passage_idx}: {e}\")\n",
    "            # Pad with zeros\n",
    "            variant_scores[variant_name].extend([0.0] * len(sentences))\n",
    "    \n",
    "    # Collect labels once\n",
    "    if passage_idx == 0 or len(all_labels) < sum(len(variant_scores[name]) for name in variants.keys()) / len(variants):\n",
    "        all_labels.extend(annotations)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "for variant_name in variant_scores:\n",
    "    variant_scores[variant_name] = np.array(variant_scores[variant_name])\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(f\"\\nEvaluation complete!\")\n",
    "print(f\"Total sentences: {len(all_labels)}\")\n",
    "print(f\"Accurate: {np.sum(all_labels == 0)}, Inaccurate: {np.sum(all_labels == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "Plot Receiver Operating Characteristic curves for all three variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = {'Shogenji': 'blue', 'Fitelson': 'green', 'Olsson': 'red'}\n",
    "\n",
    "for variant_name, scores in variant_scores.items():\n",
    "    fpr, tpr, _ = roc_curve(all_labels, scores)\n",
    "    auc_roc = roc_auc_score(all_labels, scores)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr, tpr,\n",
    "        label=f'{variant_name} (AUC={auc_roc:.3f})',\n",
    "        color=colors[variant_name],\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "# Diagonal reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Baseline')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves: Coherence-Based Hallucination Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curves\n",
    "\n",
    "Plot Precision-Recall curves showing performance at different operating points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for variant_name, scores in variant_scores.items():\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, scores)\n",
    "    auc_pr = average_precision_score(all_labels, scores)\n",
    "    \n",
    "    plt.plot(\n",
    "        recall, precision,\n",
    "        label=f'{variant_name} (AP={auc_pr:.3f})',\n",
    "        color=colors[variant_name],\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "# Baseline\n",
    "baseline = np.sum(all_labels == 1) / len(all_labels)\n",
    "plt.axhline(y=baseline, color='k', linestyle='--', linewidth=1, label=f'Random Baseline ({baseline:.3f})')\n",
    "\n",
    "# Add reference line for SelfCheckAPIPrompt baseline (93.42 AP)\n",
    "plt.axhline(y=0.9342, color='purple', linestyle=':', linewidth=2, label='SelfCheckAPIPrompt Baseline (0.934)')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves: Coherence-Based Hallucination Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower left', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distributions by Ground Truth Label\n",
    "\n",
    "Visualize how well each variant separates accurate from inaccurate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (variant_name, scores) in enumerate(variant_scores.items()):\n",
    "    # Separate scores by label\n",
    "    accurate_scores = scores[all_labels == 0]\n",
    "    inaccurate_scores = scores[all_labels == 1]\n",
    "    \n",
    "    # Plot histograms\n",
    "    axes[idx].hist(\n",
    "        accurate_scores,\n",
    "        bins=30,\n",
    "        alpha=0.6,\n",
    "        color='green',\n",
    "        label=f'Accurate (n={len(accurate_scores)})',\n",
    "        density=True\n",
    "    )\n",
    "    axes[idx].hist(\n",
    "        inaccurate_scores,\n",
    "        bins=30,\n",
    "        alpha=0.6,\n",
    "        color='red',\n",
    "        label=f'Inaccurate (n={len(inaccurate_scores)})',\n",
    "        density=True\n",
    "    )\n",
    "    \n",
    "    axes[idx].set_xlabel('Hallucination Score', fontsize=11)\n",
    "    axes[idx].set_ylabel('Density', fontsize=11)\n",
    "    axes[idx].set_title(f'{variant_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(fontsize=10)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Add mean lines\n",
    "    axes[idx].axvline(\n",
    "        np.mean(accurate_scores),\n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        label=f'Mean Accurate: {np.mean(accurate_scores):.3f}'\n",
    "    )\n",
    "    axes[idx].axvline(\n",
    "        np.mean(inaccurate_scores),\n",
    "        color='red',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        label=f'Mean Inaccurate: {np.mean(inaccurate_scores):.3f}'\n",
    "    )\n",
    "\n",
    "plt.suptitle('Hallucination Score Distributions by Ground Truth', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print separation statistics\n",
    "print(\"\\nSeparation Statistics (Mean Inaccurate - Mean Accurate):\")\n",
    "for variant_name, scores in variant_scores.items():\n",
    "    accurate_mean = np.mean(scores[all_labels == 0])\n",
    "    inaccurate_mean = np.mean(scores[all_labels == 1])\n",
    "    separation = inaccurate_mean - accurate_mean\n",
    "    print(f\"  {variant_name}: {separation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Sentence Analysis: Interesting Cases\n",
    "\n",
    "Examine cases where variants disagree or show unusual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cases where variants disagree significantly\n",
    "shogenji_scores = variant_scores['Shogenji']\n",
    "fitelson_scores = variant_scores['Fitelson']\n",
    "olsson_scores = variant_scores['Olsson']\n",
    "\n",
    "# Calculate score differences\n",
    "shog_fitel_diff = np.abs(shogenji_scores - fitelson_scores)\n",
    "shog_olsson_diff = np.abs(shogenji_scores - olsson_scores)\n",
    "fitel_olsson_diff = np.abs(fitelson_scores - olsson_scores)\n",
    "\n",
    "max_diff = np.maximum(np.maximum(shog_fitel_diff, shog_olsson_diff), fitel_olsson_diff)\n",
    "\n",
    "# Find top 10 disagreement cases\n",
    "disagreement_indices = np.argsort(max_diff)[-10:][::-1]\n",
    "\n",
    "print(\"Top 10 Cases with Highest Variant Disagreement:\\n\")\n",
    "print(f\"{'Index':<8} {'Label':<10} {'Shogenji':<12} {'Fitelson':<12} {'Olsson':<12} {'Max Diff':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx in disagreement_indices:\n",
    "    label_str = \"Accurate\" if all_labels[idx] == 0 else \"Inaccurate\"\n",
    "    print(\n",
    "        f\"{idx:<8} {label_str:<10} \"\n",
    "        f\"{shogenji_scores[idx]:<12.4f} \"\n",
    "        f\"{fitelson_scores[idx]:<12.4f} \"\n",
    "        f\"{olsson_scores[idx]:<12.4f} \"\n",
    "        f\"{max_diff[idx]:<12.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find false negatives (inaccurate sentences with low scores)\n",
    "inaccurate_indices = np.where(all_labels == 1)[0]\n",
    "avg_scores = (shogenji_scores + fitelson_scores + olsson_scores) / 3\n",
    "\n",
    "false_negative_candidates = [(idx, avg_scores[idx]) for idx in inaccurate_indices]\n",
    "false_negative_candidates.sort(key=lambda x: x[1])  # Sort by score (ascending)\n",
    "\n",
    "print(\"\\nTop 5 Potential False Negatives (Inaccurate with Low Scores):\\n\")\n",
    "print(f\"{'Index':<8} {'Avg Score':<12} {'Shogenji':<12} {'Fitelson':<12} {'Olsson':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, avg_score in false_negative_candidates[:5]:\n",
    "    print(\n",
    "        f\"{idx:<8} {avg_score:<12.4f} \"\n",
    "        f\"{shogenji_scores[idx]:<12.4f} \"\n",
    "        f\"{fitelson_scores[idx]:<12.4f} \"\n",
    "        f\"{olsson_scores[idx]:<12.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find false positives (accurate sentences with high scores)\n",
    "accurate_indices = np.where(all_labels == 0)[0]\n",
    "\n",
    "false_positive_candidates = [(idx, avg_scores[idx]) for idx in accurate_indices]\n",
    "false_positive_candidates.sort(key=lambda x: x[1], reverse=True)  # Sort by score (descending)\n",
    "\n",
    "print(\"\\nTop 5 Potential False Positives (Accurate with High Scores):\\n\")\n",
    "print(f\"{'Index':<8} {'Avg Score':<12} {'Shogenji':<12} {'Fitelson':<12} {'Olsson':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, avg_score in false_positive_candidates[:5]:\n",
    "    print(\n",
    "        f\"{idx:<8} {avg_score:<12.4f} \"\n",
    "        f\"{shogenji_scores[idx]:<12.4f} \"\n",
    "        f\"{fitelson_scores[idx]:<12.4f} \"\n",
    "        f\"{olsson_scores[idx]:<12.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Baseline\n",
    "\n",
    "Compare coherence variants to the SelfCheckAPIPrompt (GPT-3.5) baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline metrics from paper\n",
    "baseline_auc_pr = 0.9342\n",
    "baseline_pcc = 0.7832\n",
    "\n",
    "# Calculate metrics for each variant\n",
    "variant_metrics = {}\n",
    "for variant_name, scores in variant_scores.items():\n",
    "    from scipy.stats import pearsonr\n",
    "    \n",
    "    auc_pr = average_precision_score(all_labels, scores)\n",
    "    pcc, _ = pearsonr(scores, all_labels)\n",
    "    auc_roc = roc_auc_score(all_labels, scores)\n",
    "    \n",
    "    variant_metrics[variant_name] = {\n",
    "        'auc_pr': auc_pr,\n",
    "        'pcc': pcc,\n",
    "        'auc_roc': auc_roc\n",
    "    }\n",
    "\n",
    "# Create comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "variants_list = list(variant_metrics.keys()) + ['Baseline (GPT-3.5)']\n",
    "x = np.arange(len(variants_list))\n",
    "\n",
    "# AUC-PR comparison\n",
    "auc_pr_values = [variant_metrics[v]['auc_pr'] * 100 for v in variant_metrics.keys()] + [baseline_auc_pr * 100]\n",
    "bars1 = axes[0].bar(x, auc_pr_values, color=['blue', 'green', 'red', 'purple'], alpha=0.7)\n",
    "axes[0].set_ylabel('AUC-PR (%)', fontsize=12)\n",
    "axes[0].set_title('AUC-PR Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(variants_list, rotation=15, ha='right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].axhline(y=baseline_auc_pr * 100, color='purple', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# PCC comparison\n",
    "pcc_values = [variant_metrics[v]['pcc'] * 100 for v in variant_metrics.keys()] + [baseline_pcc * 100]\n",
    "bars2 = axes[1].bar(x, pcc_values, color=['blue', 'green', 'red', 'purple'], alpha=0.7)\n",
    "axes[1].set_ylabel('PCC (%)', fontsize=12)\n",
    "axes[1].set_title('Pearson Correlation Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(variants_list, rotation=15, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].axhline(y=baseline_pcc * 100, color='purple', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.suptitle('Coherence Variants vs SelfCheckAPIPrompt Baseline', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\nDetailed Metrics Comparison:\\n\")\n",
    "print(f\"{'Variant':<20} {'AUC-PR':<12} {'vs Baseline':<15} {'PCC':<12} {'AUC-ROC':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for variant_name, metrics in variant_metrics.items():\n",
    "    diff_pr = (metrics['auc_pr'] - baseline_auc_pr) * 100\n",
    "    print(\n",
    "        f\"{variant_name:<20} \"\n",
    "        f\"{metrics['auc_pr']*100:<12.2f} \"\n",
    "        f\"{diff_pr:+.2f}%{' ':<10} \"\n",
    "        f\"{metrics['pcc']*100:<12.2f} \"\n",
    "        f\"{metrics['auc_roc']*100:<12.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    f\"{'Baseline (GPT-3.5)':<20} \"\n",
    "    f\"{baseline_auc_pr*100:<12.2f} \"\n",
    "    f\"{'---':<15} \"\n",
    "    f\"{baseline_pcc*100:<12.2f} \"\n",
    "    f\"{'---':<12}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "Key observations from the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Best performing variant\n",
    "best_variant = max(variant_metrics.items(), key=lambda x: x[1]['auc_pr'])\n",
    "print(f\"\\nBest Performing Variant (by AUC-PR): {best_variant[0]}\")\n",
    "print(f\"  AUC-PR: {best_variant[1]['auc_pr']*100:.2f}%\")\n",
    "print(f\"  Improvement over baseline: {(best_variant[1]['auc_pr'] - baseline_auc_pr)*100:+.2f}%\")\n",
    "\n",
    "# Consistency across variants\n",
    "auc_pr_std = np.std([m['auc_pr'] for m in variant_metrics.values()])\n",
    "print(f\"\\nVariant Consistency (AUC-PR std): {auc_pr_std*100:.2f}%\")\n",
    "\n",
    "# Cache efficiency\n",
    "if 'cache_stats' in eval_results:\n",
    "    print(\"\\nCache Efficiency:\")\n",
    "    for variant_name, stats in eval_results['cache_stats'].items():\n",
    "        print(f\"  {variant_name}: {stats['hit_rate']*100:.2f}% hit rate\")\n",
    "\n",
    "# Cost estimates\n",
    "if 'cost_estimates' in eval_results:\n",
    "    print(\"\\nCost Estimates:\")\n",
    "    total_cost = 0\n",
    "    for variant_name, costs in eval_results['cost_estimates'].items():\n",
    "        cost = costs['estimated_cost_usd']\n",
    "        total_cost += cost\n",
    "        print(f\"  {variant_name}: ${cost:.4f} USD\")\n",
    "    print(f\"  Total: ${total_cost:.4f} USD\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided comprehensive visualization and analysis of the coherence-based hallucination detection variants. The results show:\n",
    "\n",
    "1. **Performance**: How each variant compares to the SelfCheckAPIPrompt baseline\n",
    "2. **Discrimination**: How well variants separate accurate from inaccurate sentences\n",
    "3. **Consistency**: Where variants agree or disagree in their assessments\n",
    "4. **Efficiency**: Cache hit rates and API cost management\n",
    "\n",
    "For production deployment, consider:\n",
    "- The best-performing variant based on your metric priorities (AUC-PR vs PCC)\n",
    "- API cost constraints and caching strategies\n",
    "- Ensemble approaches combining multiple variants for robustness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
