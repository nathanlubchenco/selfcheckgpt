{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelfCheckGPT Coherence Variants - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the three coherence-based hallucination detection variants:\n",
    "1. **SelfCheckShogenji** - Ratio-based independence measure (C2)\n",
    "2. **SelfCheckFitelson** - Confirmation-based support measure\n",
    "3. **SelfCheckOlsson** - Relative overlap measure (C1)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure you have set your OpenAI API key:\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"your-api-key-here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from selfcheckgpt.modeling_coherence import SelfCheckShogenji, SelfCheckFitelson, SelfCheckOlsson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Coherence Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all three coherence variants with gpt-4o-mini model\n",
    "selfcheck_shogenji = SelfCheckShogenji(model=\"gpt-4o-mini\")\n",
    "selfcheck_fitelson = SelfCheckFitelson(model=\"gpt-4o-mini\")\n",
    "selfcheck_olsson = SelfCheckOlsson(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 1: High Coherence (Truthful Statements)\n",
    "\n",
    "We expect low hallucination scores for factual statements that are consistent across samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original truthful passage\n",
    "truthful_passage = \"\"\"\n",
    "Paris is the capital of France. It is known for the Eiffel Tower, which was built in 1889. \n",
    "The city is located on the Seine River. Paris is often called the City of Light.\n",
    "\"\"\".replace(\"\\n\", \" \").strip()\n",
    "\n",
    "# Stochastically sampled passages (alternative phrasings of same facts)\n",
    "truthful_samples = [\n",
    "    \"Paris, the capital city of France, is famous for the Eiffel Tower constructed in 1889. The city sits on the banks of the Seine River.\",\n",
    "    \"The French capital Paris is home to the iconic Eiffel Tower from 1889. Paris is situated along the Seine River and nicknamed the City of Light.\",\n",
    "    \"France's capital is Paris, where you'll find the Eiffel Tower built in 1889. The Seine River runs through Paris, also known as the City of Light.\"\n",
    "]\n",
    "\n",
    "print(\"Original passage:\")\n",
    "print(truthful_passage)\n",
    "print(\"\\nSample passages:\")\n",
    "for i, sample in enumerate(truthful_samples, 1):\n",
    "    print(f\"{i}. {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize into sentences using spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "truthful_sentences = [sent.text.strip() for sent in nlp(truthful_passage).sents if len(sent) > 3]\n",
    "\n",
    "print(f\"\\nEvaluating {len(truthful_sentences)} sentences:\")\n",
    "for i, sent in enumerate(truthful_sentences, 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Shogenji\n",
    "print(\"\\n=== SelfCheckShogenji ===\")\n",
    "scores_shogenji = selfcheck_shogenji.predict(\n",
    "    sentences=truthful_sentences,\n",
    "    sampled_passages=truthful_samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nHallucination scores (lower = more truthful):\")\n",
    "for i, (sent, score) in enumerate(zip(truthful_sentences, scores_shogenji), 1):\n",
    "    print(f\"{i}. [{score:.4f}] {sent}\")\n",
    "print(f\"\\nMean hallucination score: {np.mean(scores_shogenji):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Fitelson\n",
    "print(\"\\n=== SelfCheckFitelson ===\")\n",
    "scores_fitelson = selfcheck_fitelson.predict(\n",
    "    sentences=truthful_sentences,\n",
    "    sampled_passages=truthful_samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nHallucination scores (lower = more truthful):\")\n",
    "for i, (sent, score) in enumerate(zip(truthful_sentences, scores_fitelson), 1):\n",
    "    print(f\"{i}. [{score:.4f}] {sent}\")\n",
    "print(f\"\\nMean hallucination score: {np.mean(scores_fitelson):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Olsson\n",
    "print(\"\\n=== SelfCheckOlsson ===\")\n",
    "scores_olsson = selfcheck_olsson.predict(\n",
    "    sentences=truthful_sentences,\n",
    "    sampled_passages=truthful_samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nHallucination scores (lower = more truthful):\")\n",
    "for i, (sent, score) in enumerate(zip(truthful_sentences, scores_olsson), 1):\n",
    "    print(f\"{i}. [{score:.4f}] {sent}\")\n",
    "print(f\"\\nMean hallucination score: {np.mean(scores_olsson):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 2: Low Coherence (Hallucinated Statements)\n",
    "\n",
    "We expect high hallucination scores for false statements that are inconsistent across samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hallucinated passage with false information\n",
    "hallucinated_passage = \"\"\"\n",
    "Paris is the capital of Germany. It is known for the Leaning Tower, which was built in 1776. \n",
    "The city is located on the Thames River. Paris is often called the City of Eternal Sunshine.\n",
    "\"\"\".replace(\"\\n\", \" \").strip()\n",
    "\n",
    "# Samples still contain truthful information (creating inconsistency)\n",
    "# Using the same truthful samples from Test Case 1\n",
    "\n",
    "print(\"Hallucinated passage (contains false info):\")\n",
    "print(hallucinated_passage)\n",
    "print(\"\\nTruthful sample passages (creating inconsistency):\")\n",
    "for i, sample in enumerate(truthful_samples, 1):\n",
    "    print(f\"{i}. {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize hallucinated sentences\n",
    "hallucinated_sentences = [sent.text.strip() for sent in nlp(hallucinated_passage).sents if len(sent) > 3]\n",
    "\n",
    "print(f\"\\nEvaluating {len(hallucinated_sentences)} sentences:\")\n",
    "for i, sent in enumerate(hallucinated_sentences, 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Shogenji\n",
    "print(\"\\n=== SelfCheckShogenji ===\")\n",
    "scores_shogenji_hall = selfcheck_shogenji.predict(\n",
    "    sentences=hallucinated_sentences,\n",
    "    sampled_passages=truthful_samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nHallucination scores (higher = more hallucinated):\")\n",
    "for i, (sent, score) in enumerate(zip(hallucinated_sentences, scores_shogenji_hall), 1):\n",
    "    print(f\"{i}. [{score:.4f}] {sent}\")\n",
    "print(f\"\\nMean hallucination score: {np.mean(scores_shogenji_hall):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Fitelson\n",
    "print(\"\\n=== SelfCheckFitelson ===\")\n",
    "scores_fitelson_hall = selfcheck_fitelson.predict(\n",
    "    sentences=hallucinated_sentences,\n",
    "    sampled_passages=truthful_samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nHallucination scores (higher = more hallucinated):\")\n",
    "for i, (sent, score) in enumerate(zip(hallucinated_sentences, scores_fitelson_hall), 1):\n",
    "    print(f\"{i}. [{score:.4f}] {sent}\")\n",
    "print(f\"\\nMean hallucination score: {np.mean(scores_fitelson_hall):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Olsson\n",
    "print(\"\\n=== SelfCheckOlsson ===\")\n",
    "scores_olsson_hall = selfcheck_olsson.predict(\n",
    "    sentences=hallucinated_sentences,\n",
    "    sampled_passages=truthful_samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nHallucination scores (higher = more hallucinated):\")\n",
    "for i, (sent, score) in enumerate(zip(hallucinated_sentences, scores_olsson_hall), 1):\n",
    "    print(f\"{i}. [{score:.4f}] {sent}\")\n",
    "print(f\"\\nMean hallucination score: {np.mean(scores_olsson_hall):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Variants\n",
    "\n",
    "Compare how well each variant discriminates between truthful and hallucinated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compare mean scores\n",
    "variants = ['Shogenji', 'Fitelson', 'Olsson']\n",
    "truthful_means = [\n",
    "    np.mean(scores_shogenji),\n",
    "    np.mean(scores_fitelson),\n",
    "    np.mean(scores_olsson)\n",
    "]\n",
    "hallucinated_means = [\n",
    "    np.mean(scores_shogenji_hall),\n",
    "    np.mean(scores_fitelson_hall),\n",
    "    np.mean(scores_olsson_hall)\n",
    "]\n",
    "\n",
    "x = np.arange(len(variants))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, truthful_means, width, label='Truthful', color='green', alpha=0.7)\n",
    "rects2 = ax.bar(x + width/2, hallucinated_means, width, label='Hallucinated', color='red', alpha=0.7)\n",
    "\n",
    "ax.set_ylabel('Mean Hallucination Score', fontsize=12)\n",
    "ax.set_title('Coherence Variants: Truthful vs Hallucinated Content', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(variants, fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSeparation (Hallucinated - Truthful):\")\n",
    "for i, variant in enumerate(variants):\n",
    "    separation = hallucinated_means[i] - truthful_means[i]\n",
    "    print(f\"{variant}: {separation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Wiki Bio GPT3 Hallucination Dataset (Small Subset)\n",
    "\n",
    "Test on a small subset (5 passages) from the actual evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"potsawee/wiki_bio_gpt3_hallucination\")[\"evaluation\"]\n",
    "\n",
    "# Take first 5 passages for quick testing\n",
    "num_test_passages = 5\n",
    "test_subset = dataset.select(range(num_test_passages))\n",
    "\n",
    "print(f\"Loaded {num_test_passages} passages from wiki_bio_gpt3_hallucination dataset\")\n",
    "print(f\"\\nExample passage:\")\n",
    "print(f\"Passage: {test_subset[0]['wiki_bio_text'][:200]}...\")\n",
    "print(f\"Has samples: {len(test_subset[0]['gpt3_sentences'])} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate first passage with all three variants\n",
    "idx = 0\n",
    "passage = test_subset[idx]['wiki_bio_text']\n",
    "gpt3_text = test_subset[idx]['gpt3_text']\n",
    "sentences = test_subset[idx]['gpt3_sentences']\n",
    "annotations = test_subset[idx]['annotation']\n",
    "\n",
    "# Generate samples using spacy sentence splitting on GPT-3 text\n",
    "# In a real scenario, these would be stochastic samples from the LLM\n",
    "# For this demo, we'll create variations of the passage\n",
    "samples = [gpt3_text] * 3  # Simplified: use same text as samples\n",
    "\n",
    "print(f\"\\nEvaluating passage {idx}:\")\n",
    "print(f\"Sentences to evaluate: {len(sentences)}\")\n",
    "print(f\"Ground truth annotations: {annotations}\")\n",
    "print(f\"\\nFirst 3 sentences:\")\n",
    "for i in range(min(3, len(sentences))):\n",
    "    label = \"[ACCURATE]\" if annotations[i] == 0 else \"[INACCURATE]\"\n",
    "    print(f\"{i+1}. {label} {sentences[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Shogenji (verbose to see caching)\n",
    "print(\"\\n=== Evaluating with SelfCheckShogenji ===\")\n",
    "scores_dataset_shogenji = selfcheck_shogenji.predict(\n",
    "    sentences=sentences,\n",
    "    sampled_passages=samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for i, (sent, score, ann) in enumerate(zip(sentences, scores_dataset_shogenji, annotations)):\n",
    "    label = \"[ACCURATE]\" if ann == 0 else \"[INACCURATE]\"\n",
    "    print(f\"{i+1}. {label} Score={score:.4f} | {sent[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Fitelson\n",
    "print(\"\\n=== Evaluating with SelfCheckFitelson ===\")\n",
    "scores_dataset_fitelson = selfcheck_fitelson.predict(\n",
    "    sentences=sentences,\n",
    "    sampled_passages=samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for i, (sent, score, ann) in enumerate(zip(sentences, scores_dataset_fitelson, annotations)):\n",
    "    label = \"[ACCURATE]\" if ann == 0 else \"[INACCURATE]\"\n",
    "    print(f\"{i+1}. {label} Score={score:.4f} | {sent[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Olsson\n",
    "print(\"\\n=== Evaluating with SelfCheckOlsson ===\")\n",
    "scores_dataset_olsson = selfcheck_olsson.predict(\n",
    "    sentences=sentences,\n",
    "    sampled_passages=samples,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for i, (sent, score, ann) in enumerate(zip(sentences, scores_dataset_olsson, annotations)):\n",
    "    label = \"[ACCURATE]\" if ann == 0 else \"[INACCURATE]\"\n",
    "    print(f\"{i+1}. {label} Score={score:.4f} | {sent[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Score Distribution by Ground Truth Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate scores by annotation (0=accurate, 1=inaccurate)\n",
    "accurate_scores_shog = [s for s, a in zip(scores_dataset_shogenji, annotations) if a == 0]\n",
    "inaccurate_scores_shog = [s for s, a in zip(scores_dataset_shogenji, annotations) if a == 1]\n",
    "\n",
    "accurate_scores_fitel = [s for s, a in zip(scores_dataset_fitelson, annotations) if a == 0]\n",
    "inaccurate_scores_fitel = [s for s, a in zip(scores_dataset_fitelson, annotations) if a == 1]\n",
    "\n",
    "accurate_scores_ols = [s for s, a in zip(scores_dataset_olsson, annotations) if a == 0]\n",
    "inaccurate_scores_ols = [s for s, a in zip(scores_dataset_olsson, annotations) if a == 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Shogenji\n",
    "axes[0].hist([accurate_scores_shog, inaccurate_scores_shog], \n",
    "             label=['Accurate', 'Inaccurate'], bins=10, alpha=0.7, color=['green', 'red'])\n",
    "axes[0].set_xlabel('Hallucination Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('SelfCheckShogenji')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Fitelson\n",
    "axes[1].hist([accurate_scores_fitel, inaccurate_scores_fitel], \n",
    "             label=['Accurate', 'Inaccurate'], bins=10, alpha=0.7, color=['green', 'red'])\n",
    "axes[1].set_xlabel('Hallucination Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('SelfCheckFitelson')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Olsson\n",
    "axes[2].hist([accurate_scores_ols, inaccurate_scores_ols], \n",
    "             label=['Accurate', 'Inaccurate'], bins=10, alpha=0.7, color=['green', 'red'])\n",
    "axes[2].set_xlabel('Hallucination Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('SelfCheckOlsson')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScore Statistics:\")\n",
    "print(f\"Shogenji - Accurate: mean={np.mean(accurate_scores_shog):.4f}, Inaccurate: mean={np.mean(inaccurate_scores_shog):.4f}\")\n",
    "print(f\"Fitelson - Accurate: mean={np.mean(accurate_scores_fitel):.4f}, Inaccurate: mean={np.mean(inaccurate_scores_fitel):.4f}\")\n",
    "print(f\"Olsson - Accurate: mean={np.mean(accurate_scores_ols):.4f}, Inaccurate: mean={np.mean(inaccurate_scores_ols):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Basic usage of all three coherence variants\n",
    "2. Behavior on known truthful vs hallucinated content\n",
    "3. Comparison of variant discrimination capabilities\n",
    "4. Testing on real dataset examples\n",
    "5. Cache efficiency and API cost management\n",
    "\n",
    "For comprehensive evaluation on the full dataset, see `scripts/evaluate_coherence.py` and `demo/coherence_evaluation.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
